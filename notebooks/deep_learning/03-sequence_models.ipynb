{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling sequences with RNNs\n",
    "\n",
    "When we go from sequences of word embeddings to a document-wise vector representation that can be classified, we have to somehow summarize a sequence of vectors into a single vector. So far, what we have been doing is:\n",
    "\n",
    "1. Get one embedding $e \\in \\mathbb{R}^{l \\times d}$ per token, where  $l$ is the sequence length and $d$ is the embedding dimension. This generates the embedding matrix $E \\in \\mathbb{R}^{b \\times l \\times d}$, where $b$ is the batch size.\n",
    "2. Calculate the timewise mean of $E$, generating $X \\in \\mathbb{R}^{b \\times d}$\n",
    "3. Proceed to classification with our logist regression.\n",
    "\n",
    "This is something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "\n",
    "    subgraph Embedding;\n",
    "    D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
    "    end;\n",
    "\n",
    "    subgraph Summarization;\n",
    "    D --> E[\"Mean over time\"] --> F([\"Embeddings ($$X \\in \\mathbb{R}^{b \\times d}$$)\"])\n",
    "    end;\n",
    "\n",
    "    subgraph Classification;\n",
    "    F --> G[\"Logistic Regression\"] --> H([\"$$P(C_i=c_j | X_i)$$\"])\n",
    "    end;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this idea is that the calculation of the mean totally disregards the order of the words - essentially, we are doing a glorified bag-of-words modelling, which seems non-ideal. When we do so, we are We could find some other way to summarize our sequence of words so that we somehow account for the order of words.\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) were diffusely invented by many small contributions during the 1950s to the 1970s. The underlying idea is to begin with a simple logistic regression that receives as input the first time step of $E$, and yields a single prediction $y_0$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    F([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"]) -->\n",
    "    FF([\"$$x_e = E_{[0,:,:]}$$\"])\n",
    "    FF --> G[\"Logistic Regression\"] --> H([$$y_0$$])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next prediction, we concatenate the input at step $1$ with the output at the step $0$, that is, we use $x_e = [E_{[1,:,:]}, y_0]$ and get another prediction $y_1$. Then we keep doing this for all time steps $t \\in [0, l-1]$. The network starts looking like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    F([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
    "    FF([\"$$x_e = [E_{[t,:,:]}, y_{t-1}]$$\"])\n",
    "\n",
    "    F-->FF\n",
    "    FF --> G[\"Logistic Regression\"] \n",
    "\n",
    "    G --> H([$$y_t$$])\n",
    "\n",
    "    H -- \"Feedback loop\" --> FF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that logistic regression works by using $y=\\sigma(z)$, where $z = xw^t+b$. We can change the non-linear function for this purpose - usually, the applied function is $\\tanh(z)$. The hyperbolic tangent $\\tanh$ can be calculated by:\n",
    "\n",
    "$$\n",
    "\\tanh(z) = 2\\sigma(z)-1.\n",
    "$$\n",
    "\n",
    "The advantage of $\\tanh$ is that it can assume negative values, which can be useful for optimization. However, when we use $\\tanh$ instead of $\\sigma$, we no longer have a logistic regression. Instead, we have a linear layer followed by a non-linear function, that is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    F([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
    "    FF([\"$$x_e = [E_{[t,:,:]}, y_{t-1}]$$\"])\n",
    "\n",
    "    F-->FF\n",
    "    FF --> G[\"$$\\tanh(xw^T+b)$$\"] \n",
    "\n",
    "    G --> H([$$y_t$$])\n",
    "\n",
    "    H -- \"Feedback loop\" --> FF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The output $y$ is often called \"hidden state\" and referred to as $h$. This is a reference to the fact that this output is usually a part of a larger classifier, and is hidden because it is an intermediate result of the network (similarly to the intermediate results in a deep MLP).\n",
    "\n",
    "\n",
    "\n",
    "Typically, RNNs are used to summarize sequences by propagating the last output, that is, our summarization of a sequence of \n",
    " elements is simply $X=y_l$.\n",
    "\n",
    "Now, note that we have no restriction as to what the dimension of $y$ should be. We could choose to yield, for example, $y_t$ with 50 dimensions, or 500 dimensions. This choice would simply imply in greater dimensions for $x$ and more degrees of freedom in the logistic regression. If this dimension is $d_e$, then $X \\in \\mathbb{R}^{b \\times d_e}.$\n",
    "\n",
    "Hence, our classifier now works like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "\n",
    "    subgraph Embedding;\n",
    "    D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
    "    end;\n",
    "\n",
    "    subgraph Summarization;\n",
    "    D-->FF([\"$$x_e = [E_{[t,:,:]}, y_{t-1}]$$\"])\n",
    "\n",
    "    FF --> GF[\"$$\\tanh(xw^T+b)$$\"] \n",
    "\n",
    "    GF --> HF([$$y_t$$])\n",
    "\n",
    "    HF -- \"Feedback loop\" --> FF\n",
    "    HF -- \"Last\n",
    "    State\" --> F([\"Embeddings ($$X \\in \\mathbb{R}^{b \\times d_h}$$)\"])\n",
    "    end;\n",
    "\n",
    "    subgraph Classification;\n",
    "    F --> G[\"Logistic Regression\"] --> H([\"$$P(C_i=c_j | X_i)$$\"])\n",
    "    end;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pytorch implementation of RNN is simply a layer like:\n",
    "\n",
    "    rnnlayer = nn.RNN(input_size, hidden_size)\n",
    "\n",
    "We could change our network from the last lesson to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 5000\n",
    "padding_idx = 3\n",
    "hidden_dim = 20 # this is dh\n",
    "embedding_dim = 30 # this is d\n",
    "\n",
    "class ClassifierWithRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.rnnlayer = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.clf = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h, _ = self.rnnlayer(x)\n",
    "        x = h[:,-1,:]\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "model = ClassifierWithRNN(vocab_size, hidden_dim, embedding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: What happens to embeddings during training?\n",
    "\n",
    "We will now put everything to use and make some scientific experiments. Feel free to work in groups.\n",
    "\n",
    "The question is: **what happens to embeddings while we train our network**?\n",
    "\n",
    "Follow the steps below to find out.\n",
    "\n",
    "1. Add a method in `ClassifierWithRNN` that receives a token sequence as input (similarly to forward) but returns the result of the summarization.\n",
    "2. Download the FakeNewsNet dataset from Kagglehub (code is below)\n",
    "3. Make a `ClassifierWithRNN` with a hidden dimension of $2$. \n",
    "4. Calculate document-level embeddings for the elements of the dataset *before* training and show them in a scatter plot. Use colors to represent the classes of each item.\n",
    "5. Train your classifier.\n",
    "6. Calculate document-level embeddings for the elements of the dataset *after* training and show them in a scatter plot. Use colors to represent the classes of each item.\n",
    "\n",
    "What could you observe? Why do you think that is happening?\n",
    "\n",
    "Now, do it again using the summarization based on mean. What do you observe? Why is that happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "print(\"FakeNewsNet\")\n",
    "fnnpath = kagglehub.dataset_download(\"algord/fake-news\")\n",
    "print(\"Path to dataset files:\", fnnpath)\n",
    "files = os.listdir(fnnpath)\n",
    "print(\"Files in dataset path:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with this pre-trained tokenizer, so you dont have to train your own:\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('fakenews_tokenizer.model')\n",
    "padding_idx = sp.piece_to_id('<PAD>')\n",
    "\n",
    "def pad_to_len(sequences, pad_idx, max_len):\n",
    "    padded = []\n",
    "    for s in sequences:\n",
    "        if len(s) >= max_len:\n",
    "            padded.append(s[:max_len])\n",
    "        else:\n",
    "            padded.append(s + [pad_idx] * (max_len - len(s)))\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my solution. DO NOT LOOK AT IT before trying yours.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassifierWithRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.rnnlayer = nn.RNN(embedding_dim, hidden_dim, batch_first=True, num_layers=1)\n",
    "        self.clf = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def summarize(self, x):\n",
    "        x, _ = self.rnnlayer(x)\n",
    "        x = x[:,-1,:]\n",
    "        return(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.summarize(x)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(Path(fnnpath) / \"FakeNewsNet.csv\")\n",
    "X = df['title']\n",
    "y = torch.tensor(df['real']).float()\n",
    "print(len(X), len(y))\n",
    "tokens = sp.encode_as_ids(list(X))\n",
    "tokens = pad_to_len(tokens, padding_idx, 30)\n",
    "tokens = torch.tensor(tokens)\n",
    "print(tokens.shape, y.shape)\n",
    "model = ClassifierWithRNN(\n",
    "    vocab_size=5000,\n",
    "    hidden_dim=2,\n",
    "    embedding_dim=2,\n",
    ").cpu()\n",
    "\n",
    "model.eval()\n",
    "embeddings_pre = model.summarize(model.embedding(tokens))\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e-3)  # lr is the learning rate - this is our alpha\n",
    "\n",
    "\n",
    "\n",
    "# And now, this is the training loop:\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model = model.cuda()\n",
    "tokens = tokens.cuda()\n",
    "y = y.cuda()\n",
    "print(\"Entering loop\")\n",
    "for epoch in tqdm(range(3000)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(tokens)\n",
    "    loss = torch.mean(\n",
    "        torch.binary_cross_entropy_with_logits(\n",
    "            output.flatten().float(),\n",
    "            y,\n",
    "        ))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "model = model.cpu()\n",
    "tokens = tokens.cpu()\n",
    "y = y.cpu()\n",
    "model.eval()\n",
    "embeddings_post = model.summarize(model.embedding(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "e1 = embeddings_pre.cpu().detach().numpy()\n",
    "#y = y.cpu().detach().numpy()\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(e1[:,0], e1[:,1], c=y, alpha=0.2)\n",
    "plt.title(\"Before training\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(embeddings_post.detach().cpu().numpy()[:,0], embeddings_post.detach().cpu().numpy()[:,1], c=y, alpha=0.2)\n",
    "plt.title(\"After training\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_post[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "If everything went well, you should know how to:\n",
    "\n",
    "1. Make a PyTorch model\n",
    "1. Use RNNs to summarize the content of sequences\n",
    "1. Train neural networks with RNNs\n",
    "1. Use the GPU to speed-up training\n",
    "1. Visualize the learning curve and diagnose failures with the learning rate\n",
    "1. Visualize document-level embeddings to check the final state of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
