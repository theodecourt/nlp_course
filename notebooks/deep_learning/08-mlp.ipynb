{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb33336d",
   "metadata": {},
   "source": [
    "# 08 - MLPs and Residual propagation\n",
    "\n",
    "So far, we have been using Logistic Regression for all our classification needs. Logistic regression is very similar to linear regression, except for that $\\sigma(z)$ in the end - it is essentially a linear projection and a choice between the \"positive\" and the \"negative\" sides of the projection surface.\n",
    "\n",
    "Also, we have seen that we can choose to project our data $X$ into an intermediate representation $z$ so that $z$ has more than one dimension. We can use that for multi-class classification.\n",
    "\n",
    "Now, we are going to view the effects of mapping the intermediate projection $z$ to another intermediate projection (let's call it $z_2$). As we will see, increasing the dimensionality of each representation $z_i$ and the number of intermediate projections has the effect of creating intermediate regions in which we can apply linear transformations separately. If you want a theoretical reference for such, refer to: [Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4558-4566, 2018.](https://proceedings.mlr.press/v80/serra18b/serra18b.pdf).\n",
    "\n",
    "In the examples shown here, we will start with the use of Linear Regression to show some limitations of this approach. These animated examples work like this:\n",
    "\n",
    "1. We define a random dataset $X$\n",
    "1. We define a target $y$ by applying some function over $X$\n",
    "1. We initialize a prediction model with an identity function (that is, our weights are such that )\n",
    "1. We train the prediction model to predict $\\hat{y} = f(X)$ using gradient descent, and store $\\hat{y}_t$ for each iteration $t$\n",
    "1. We make an animation of all $\\hat{y}_t$ so we can see what happens to our predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b44f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, lr=0.01, epochs=100):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    outputs = [X.numpy()]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Forward pass\n",
    "        predictions = model(X)\n",
    "        outputs.append(predictions.detach().numpy())\n",
    "        loss = criterion(predictions, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model, outputs\n",
    "\n",
    "def animate_training(outputs, y, frame_duration=5):\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "\n",
    "    # Convert outputs to a pandas DataFrame for easier plotting\n",
    "    frames = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        df = pd.DataFrame(output, columns=['Feature 1', 'Feature 2'])\n",
    "        df['Frame'] = i  # Add a frame identifier\n",
    "        frames.append(df)\n",
    "\n",
    "    total_frames = len(frames)\n",
    "    n = total_frames // 100\n",
    "    frames = frames[::n]  # Get every nth frame\n",
    "    \n",
    "    # Concatenate all frames into a single DataFrame\n",
    "    animated_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "    fig = px.scatter(\n",
    "\n",
    "        animated_df,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        x='Feature 1',\n",
    "        y='Feature 2',\n",
    "        animation_frame='Frame',\n",
    "        title='Outputs Animation',\n",
    "        labels={'Feature 1': 'Feature 1', 'Feature 2': 'Feature 2'}\n",
    "    )\n",
    "\n",
    "    # Add a scatterplot of y\n",
    "    scatter_y = pd.DataFrame(y.numpy(), columns=['Feature 1', 'Feature 2'])\n",
    "    scatter_y['Frame'] = -1  # Use -1 to indicate the original data\n",
    "    for _, row in scatter_y.iterrows():\n",
    "        fig.add_trace(px.scatter(\n",
    "            pd.DataFrame([row]),\n",
    "            x='Feature 1',\n",
    "            y='Feature 2',\n",
    "            color='Frame',\n",
    "\n",
    "        ).data[0])\n",
    "\n",
    "    # Adjust animation speed\n",
    "    fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = frame_duration  # Set duration in milliseconds\n",
    "    fig.update_layout(coloraxis_showscale=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33333369",
   "metadata": {},
   "source": [
    "## A simple dataset: a rotation + translation\n",
    "\n",
    "A linear regression is capable to find the correct rotation and translation of a dataset. This is because rotations and translations can be immediately expressed by the linear prediction equation $y = xw^t + b$ - in this case, the weight matrix $w$ can be constructed from a rotation, and $b$ corresponnds to the translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock dataset\n",
    "X = torch.randn(100, 2) * 5  # 100 samples, 2 feature\n",
    "theta = torch.tensor(30.0 * torch.pi / 180.0)  # Convert degrees to radians\n",
    "rotation_matrix = torch.tensor([\n",
    "    [torch.cos(theta), -torch.sin(theta)],\n",
    "    [torch.sin(theta), torch.cos(theta)]\n",
    "])\n",
    "y = X @ rotation_matrix + torch.tensor([5,5]) + 0.01 * torch.randn(100, 2)  # y = 3x + 2 + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9c5f3",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d258840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "linear_model = nn.Linear(\n",
    "    in_features=2,\n",
    "    out_features=2,\n",
    ")\n",
    "linear_model.weight.data = torch.eye(2)  # Initializing with identity\n",
    "linear_model.bias.data = torch.zeros(2)  # Initializing bias to zero\n",
    "\n",
    "model, outputs = train_model(\n",
    "    model=linear_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    epochs=1000\n",
    ")\n",
    "\n",
    "fig = animate_training(outputs, y)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f98bf",
   "metadata": {},
   "source": [
    "## A more complicated dataset: linear by parts\n",
    "\n",
    "Now, let's get a more complicated dataset. Now, $X$ (our input) will have three different clusters, and we will apply a different linear transform in each cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock dataset\n",
    "X1 = torch.randn(100, 2) + torch.tensor([3,-3])   # 100 samples, 2 feature\n",
    "theta = torch.tensor(30.0 * torch.pi / 180.0)  # Convert degrees to radians\n",
    "y1 = 3*X1 + 0.01 * torch.randn(100, 2)  # y = 3x + 2 + noise\n",
    "\n",
    "# Create a mock dataset\n",
    "X2 = torch.randn(100, 2)  # 100 samples, 2 feature\n",
    "theta = torch.tensor(150.0 * torch.pi / 180.0)  # Convert degrees to radians\n",
    "rotation_matrix2 = torch.tensor([\n",
    "    [torch.cos(theta), -torch.sin(theta)],\n",
    "    [torch.sin(theta), torch.cos(theta)]\n",
    "])\n",
    "y2 = (X2 @ rotation_matrix2) + torch.tensor([-3,3]) + 0.01 * torch.randn(100, 2)  # y = 3x + 2 + noise\n",
    "\n",
    "# Create a mock dataset\n",
    "X3 = torch.randn(100, 2) + torch.tensor([3,3]) # 100 samples, 2 feature\n",
    "theta = torch.tensor(150.0 * torch.pi / 180.0)  # Convert degrees to radians\n",
    "y3 = -5*X3 - 0.01 * torch.randn(100, 2)  # y = 3x + 2 + noise\n",
    "\n",
    "\n",
    "X = torch.cat((X1, X2, X3), dim=0)\n",
    "y = torch.cat((y1, y2, y3), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b3feb",
   "metadata": {},
   "source": [
    "When we try to approximate this using a linear layer, we obviously can't. This is due to our data being more complicated than the model - or, in other words, our model is not *expressive* enough to model our data. In the animation, we clearly see that the linear layer can only apply the same transform to all points in the input vector space, hence they all \"bend\" in the same way.\n",
    "\n",
    "Our model is unable, for example, to model the different cluster variances generated by the different multiplications applied when we generated each part of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f567bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "linear_model = nn.Linear(\n",
    "    in_features=2,\n",
    "    out_features=2,\n",
    ")\n",
    "linear_model.weight.data = torch.eye(2)  # Initializing with identity\n",
    "linear_model.bias.data = torch.zeros(2)  # Initializing bias to zero\n",
    "\n",
    "model, outputs = train_model(\n",
    "    model=linear_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "fig = animate_training(outputs, y)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9723a3",
   "metadata": {},
   "source": [
    "## MLP models\n",
    "\n",
    "A possible upgrade to the linear model is the MLP model. The MLP model is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(xw_1^t+b_2)w_2^t+b_2,\n",
    "$$\n",
    "where $f$ is the Rectifying Linear Unit (ReLU) function given by $f(z)=0, z<0, f(z)=z, z>0$.\n",
    "\n",
    "We can interpret this equation as two layers of linear projections, separated by a non-linear operation, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f09b2",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "\n",
    "graph LR;\n",
    "    A((X)) --> B[\"$$\\times w_1^T$$\"];\n",
    "    subgraph Layer 1;\n",
    "    B --> C[\"\"$$+ b_1$$\"\"];\n",
    "\n",
    "    C --> D[\"\"\"f\"\"\"];\n",
    "        end;\n",
    "    subgraph Layer 2;\n",
    "    D --> E[\"$$\\times w_2^T$$\"];\n",
    "    E --> F[\"\"$$+ b_2$$\"\"];\n",
    "    end;\n",
    "    F --> G((ŷ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e115531",
   "metadata": {},
   "source": [
    "In the animation, we see that each part of the space is being folded and bended differenty. This is because of the non-linearity given by the ReLU function.\n",
    "\n",
    "### A very small example\n",
    "\n",
    "Let's suppose we have two 1-dimensional inputs: $x_1$ and $-x_2$, where $x_1$ and $x_2$ are real and positive. Our network has 1-d outputs as well.\n",
    "\n",
    "For simplicity, let's assume $b_1=b_2=0$\n",
    "\n",
    "$w_1$ will be equal to $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. Thus, $xw_1^T=\\begin{bmatrix} x_1 & -x_2 \\\\ -x_1 & x_2 \\end{bmatrix}$.\n",
    "\n",
    "Now, after applying $f(.)$ to $xw_1^T$, we have:\n",
    "\n",
    "$$\n",
    "f(xw_1^T)=\\begin{bmatrix} x_1 & 0 \\\\ 0 & x_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is important: each one of our rows are independent of each input!\n",
    "\n",
    "Now, note that $w_2$ must be 2x1 (let's say it is equal to $[c, d]$), and:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix} x_1 & 0 \\\\ 0 & x_2 \\end{bmatrix} \\begin{bmatrix} c \\\\ d \\end{bmatrix} = \\begin{bmatrix} c x_1 \\\\ d x_2 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Now, importantly: in our model, the first input received a scale of $c$, while the second input received a scale of $d$.\n",
    "\n",
    "Thus, the model operates in two layers. In the first layer, it divides the inputs into groups; in the second layer, it applies a different linear transform for each group.\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "The values for the weight and bias matrices must be trained using gradient descent. However, $f(.)$ has zero gradient for all negative inputs. For this reason, it is common to see output values organizing in straight lines - located exactly where the point of inflection is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "\n",
    "        # Initialize weights to identity and biases to zero\n",
    "        nn.init.eye_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.eye_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nonlinearity(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the MLP\n",
    "hidden_size = 2  # Example hidden layer size\n",
    "mlp_model = MLP(input_size, hidden_size, output_size)\n",
    "model, outputs = train_model(\n",
    "    model=mlp_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    epochs=5000\n",
    ")\n",
    "\n",
    "fig = animate_training(outputs, y, frame_duration=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa8df3",
   "metadata": {},
   "source": [
    "### Residual blocks\n",
    "\n",
    "The problem of zero-gradient has been tackled by many approaches. One of the most successfull was to create an alternate route for gradients to propagate. This route is called \"residual\", and involves adding the input to the output of the network, that is:\n",
    "\n",
    "```mermaid\n",
    "\n",
    "graph LR;\n",
    "    A((X)) --> B[\"$$\\times w_1^T$$\"];\n",
    "    subgraph Layer 1;\n",
    "    B --> C[\"\"$$+ b_1$$\"\"];\n",
    "\n",
    "    C --> D[\"\"\"f\"\"\"];\n",
    "        end;\n",
    "    subgraph Layer 2;\n",
    "    D --> E[\"$$\\times w_2^T$$\"];\n",
    "    E --> F[\"\"$$+ b_2$$\"\"];\n",
    "    end;\n",
    "    F --> H(\"SUM\");\n",
    "    A -->|Residual Connection| H;\n",
    "    H --> G((ŷ));\n",
    "```\n",
    "$$\n",
    "\\hat{y} = x + (f(xw_1^t+b_2)w_2^t+b_2),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "\n",
    "        # Initialize weights to identity and biases to zero\n",
    "        nn.init.eye_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.eye_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.nonlinearity(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x += residual  # Add the residual connection\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.r1 = ResidualBlock(hidden_size)\n",
    "        self.r2 = ResidualBlock(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Initialize weights to identity and biases to zero\n",
    "        nn.init.eye_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.eye_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.r2(x)\n",
    "        y = self.fc2(x)\n",
    "        return y\n",
    "\n",
    "# Create an instance of the MLP\n",
    "hidden_size = 2  # Example hidden layer size\n",
    "mlp_model = MLP(input_size, hidden_size, output_size)\n",
    "model, outputs = train_model(\n",
    "    model=mlp_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    epochs=5000\n",
    ")\n",
    "\n",
    "fig = animate_training(outputs, y, frame_duration=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee03cf",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "The non-linearities allow applying different transforms to each region of the input space. The residual connections avoids the vanishing gradient problem. Now, we add an extra layer of stability by normalizing data in each layer. Normalization helps maintaining all representations within reasonable values, which helps numerical stability and has ultimately been linked to faster convergence in neural networks.\n",
    "\n",
    "Using normalization after each layer, we do observe a faster convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "\n",
    "        # Initialize weights to identity and biases to zero\n",
    "        nn.init.eye_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.eye_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.nonlinearity(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x += residual  # Add the residual connection\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.r1 = ResidualBlock(hidden_size)\n",
    "        self.r2 = ResidualBlock(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Initialize weights to identity and biases to zero\n",
    "        nn.init.eye_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.eye_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.r1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.r2(x)\n",
    "        x = self.bn3(x)\n",
    "        y = self.fc2(x)\n",
    "        return y\n",
    "\n",
    "# Create an instance of the MLP\n",
    "hidden_size = 2  # Example hidden layer size\n",
    "mlp_model = MLP(input_size, hidden_size, output_size)\n",
    "model, outputs = train_model(\n",
    "    model=mlp_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    epochs=5000\n",
    ")\n",
    "\n",
    "fig = animate_training(outputs, y, frame_duration=1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de089b59",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Althoug [our reference](https://proceedings.mlr.press/v80/serra18b/serra18b.pdf) states that there is a theoretical upper bound for the number of regions created by subsequent ReLU-separated projections, it is still challenging to find what are the optimal regions and corresponding projections for a particular dataset.\n",
    "\n",
    "Our toolset for such is:\n",
    "\n",
    "1. We can use simple linear regressions or logistic regressions to find a baseline for our system.\n",
    "1. We can use the MLP topology to create potential regions in our dataset. More layers, and more neurons per layer, increase the *expressivity* of the network, that is, the number of linear regions it can model.\n",
    "1. Adding a residual connection helps propagating gradients to the earlier layers of the MLP, which favors using the whole potential of the network.\n",
    "1. Normalization layers help leading to a more numerically stable fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4646a6",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Make a neural network that maps $X$ to $y$ using the data below. Plot an animation of the convergence process, like the ones shown above. Try the different model variations - what happens in each case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.linspace(-1, 1, 500)\n",
    "x2 = torch.linspace(1, -1, 500)\n",
    "X = torch.stack([x1, x2], dim=1)\n",
    "X += 0.1 * torch.randn(500, 2)  # Adding noise to X\n",
    "\n",
    "y = 2*X + X**2 +0.5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(), label='X', alpha=0.5)\n",
    "plt.scatter(y[:, 0].numpy(), y[:, 1].numpy(), label='y', alpha=0.5)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Scatterplot of X and y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb29177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae860c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
